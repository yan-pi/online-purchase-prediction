# E-Commerce Purchase Intention Prediction

Machine learning pipeline for binary classification of online shopping session outcomes using Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), and Random Forest models with GridSearchCV hyperparameter tuning.

## Dataset

**UCI Online Shoppers Purchasing Intention Dataset**
- **Source**: https://archive.ics.uci.edu/dataset/468
- **Instances**: 12,330 sessions
- **Features**: 17 (10 numeric, 7 categorical)
- **Target**: Revenue (binary: purchase/no purchase)
- **Class Imbalance**: 84.5% negative, 15.5% positive
- **Paper**: Sakar et al. (2019) Neural Computing & Applications

### Feature Categories
- **Behavioral** (6): Page counts and durations (Administrative, Informational, ProductRelated)
- **Google Analytics** (3): BounceRates, ExitRates, PageValues
- **Temporal** (3): SpecialDay, Month, Weekend
- **Technical** (5): OperatingSystems, Browser, Region, TrafficType, VisitorType

## Project Structure

```
/Users/ybarbara/www/personal/online-dataset/
├── src/
│   ├── __init__.py
│   ├── data_loader.py       # UCI dataset loading
│   ├── preprocessing.py     # Feature engineering pipeline
│   ├── models.py            # MLP, SVM, RF training with GridSearchCV
│   └── evaluation.py        # Metrics and visualization
├── notebooks/
│   └── 01_eda.ipynb         # Exploratory data analysis
├── data/                    # Auto-downloaded dataset cache
├── results/                 # Model outputs (generated by main.py)
│   ├── model_comparison.csv
│   ├── confusion_matrix_mlp.png
│   ├── confusion_matrix_svm.png
│   ├── confusion_matrix_random_forest.png
│   └── roc_curves.png
├── paper/                   # LaTeX documentation
├── main.py                  # Main pipeline orchestrator
├── pyproject.toml           # Modern Python dependencies
├── mise.toml                # Development environment config
└── README.md                # This file
```

## Installation

### Prerequisites
- Python >= 3.10
- mise (development environment manager)

### Setup Steps

1. **Install mise** (if not already installed):
   ```bash
   curl https://mise.run | sh
   ```

2. **Navigate to project directory**:
   ```bash
   cd /Users/ybarbara/www/personal/online-dataset
   ```

3. **Install dependencies with mise and uv**:
   ```bash
   mise install                    # Installs uv
   mise exec -- uv venv            # Create virtual environment
   mise exec -- uv pip install -e . # Install project dependencies
   mise exec -- uv pip install -e ".[dev]"  # Install dev tools (ruff)
   ```

4. **Activate virtual environment**:
   ```bash
   source .venv/bin/activate
   ```

5. **Verify installation**:
   ```bash
   python -c "from ucimlrepo import fetch_ucirepo; print('Installation successful!')"
   ```

## Usage

### Run Complete Pipeline

```bash
source .venv/bin/activate
python main.py
```

**Expected runtime**: 20-30 minutes (GridSearchCV is computationally intensive)

**Pipeline steps**:
1. Load UCI dataset (auto-download on first run)
2. Stratified train-test split (80/20)
3. Preprocessing (label encoding, standardization)
4. Train MLP (~5-10 min)
5. Train SVM (~5-10 min)
6. Train Random Forest (~5-10 min)
7. Evaluate all models
8. Generate visualizations and CSV

### Run Exploratory Data Analysis

#### Option 1: Using Docker (Recommended)

```bash
# Build and start Jupyter notebook server
docker-compose up --build

# Access Jupyter in your browser at:
# http://localhost:8888

# Navigate to: notebooks/01_eda.ipynb
```

The notebook will be available without authentication. All changes are persisted to your local filesystem via volume mount.

To stop the server:
```bash
docker-compose down
```

#### Option 2: Using Local Python Environment

```bash
source .venv/bin/activate
jupyter notebook notebooks/01_eda.ipynb
```

Execute all cells to visualize:
- Target distribution (class imbalance)
- Feature distributions
- Correlation matrix
- Categorical feature breakdown

### Code Quality Tools

```bash
source .venv/bin/activate

# Format all Python files
ruff format .

# Check for linting issues
ruff check .

# Auto-fix issues
ruff check --fix .
```

## Models Implemented

### 1. Multi-Layer Perceptron (MLP)
- **Architecture**: GridSearchCV over hidden layers [(100,50), (100,), (50,50)]
- **Optimizer**: Adam with adaptive learning rate
- **Regularization**: L2 penalty (alpha ∈ {0.0001, 0.001})
- **Features**: Early stopping, validation-based convergence

### 2. Support Vector Machine (SVM)
- **Kernels**: RBF and Linear
- **Regularization**: C ∈ {0.1, 1, 10}
- **Gamma**: {scale, auto}
- **Features**: Class weighting for imbalanced data, probability estimates

### 3. Random Forest
- **Estimators**: {50, 100, 200} trees
- **Max Depth**: {10, 20, None}
- **Min Samples Split**: {2, 5}
- **Features**: Class weighting, built-in feature importance

### Common Configurations
- **Cross-Validation**: 5-fold stratified
- **Scoring Metric**: F1-score (better for imbalanced data)
- **Class Weighting**: Balanced (handles 84.5%/15.5% imbalance)
- **Random State**: 42 (reproducibility)

## Evaluation Metrics

All models evaluated on 5 key metrics:

- **Accuracy**: Overall classification correctness
- **Precision**: Positive predictive value (TP / (TP + FP))
- **Recall**: True positive rate (TP / (TP + FN)) - critical for imbalanced data
- **F1-Score**: Harmonic mean of precision and recall
- **AUC-ROC**: Area under ROC curve (discrimination ability)

## Expected Results

Based on dataset characteristics and class imbalance:

| Metric | Expected Range |
|--------|----------------|
| F1-Score | 0.60 - 0.75 |
| AUC-ROC | 0.85 - 0.92 |
| Recall | 0.55 - 0.70 |
| Precision | 0.65 - 0.80 |
| Accuracy | 0.80 - 0.90 |

**Note**: Actual results will be saved to `results/model_comparison.csv` after running `main.py`.

## Technical Details

### Preprocessing Pipeline
1. **Label Encoding**: Categorical features (Month, VisitorType, OperatingSystems, Browser, Region, TrafficType)
2. **Boolean Conversion**: Weekend (bool → int)
3. **Standard Scaling**: Numeric features (z-score normalization: z = (x - μ) / σ)
4. **Train-Test Split**: Stratified 80/20 (maintains class distribution)

### Hyperparameter Tuning
- **Strategy**: Exhaustive GridSearchCV
- **Cross-Validation**: 5-fold stratified
- **Scoring**: F1-score (prioritizes balance between precision and recall)
- **Parallelization**: n_jobs=-1 (utilizes all CPU cores)

### Why These Choices?

**F1-Score over Accuracy**: With 84.5% negative class, a model predicting all "no purchase" would achieve 84.5% accuracy but be useless. F1-score forces balance.

**Class Weighting**: Alternative to SMOTE; simpler and avoids synthetic data generation.

**StandardScaler**: SVM is distance-based (requires scaling); MLP converges faster with normalized inputs.

## Output Files

After running `python main.py`, check the `results/` directory:

```bash
ls -lh results/
```

**Generated files**:
- `model_comparison.csv` - Metrics table (can copy to paper LaTeX)
- `confusion_matrix_mlp.png` - MLP confusion matrix
- `confusion_matrix_svm.png` - SVM confusion matrix
- `confusion_matrix_random_forest.png` - RF confusion matrix
- `roc_curves.png` - ROC curves for all models (comparative)

## Troubleshooting

### Issue: UCI Dataset Download Fails
**Solution**: Check internet connection; dataset auto-downloads on first run.

### Issue: GridSearchCV Too Slow
**Solutions**:
- Reduce grid size in `src/models.py`
- Use `RandomizedSearchCV` instead
- Test with subset: `X_train[:5000]`

### Issue: MLP Doesn't Converge
**Solution**: Increase `max_iter` from 500 to 1000 in `src/models.py`

### Issue: Out of Memory
**Solution**: Set `n_jobs=1` in GridSearchCV calls to reduce parallelization

## Development Tools

This project uses modern Python tooling:
- **mise**: Development environment manager
- **uv**: Fast Python package installer
- **ruff**: Fast linter, formatter, and import sorter

## License

MIT License

## Citation

If using this dataset, please cite:
```
Sakar, C.O., Polat, S.O., Katircioglu, M. et al.
Real-time prediction of online shoppers' purchasing intention using multilayer perceptron and LSTM recurrent neural networks.
Neural Comput & Applic 31, 6893–6908 (2019).
https://doi.org/10.1007/s00521-018-3523-0
```
